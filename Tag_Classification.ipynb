{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tag_Classification_Younet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct3Vi23YqEq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06e4f57-7632-4afc-faab-8490bf6789d1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "from numpy import hstack\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "import swifter\n",
        "import string\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "import ast\n",
        "from Regex import *\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Uyen\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXV1gjGz8BOa"
      },
      "source": [
        "# CONSTANT VARIABLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMlrzC5t74Ps"
      },
      "source": [
        "PATH_TO_TRAIN = 'data/PRICE_MAY_NOV_2019.csv'\n",
        "PREDICTION_THRESHOLD = 0.5\n",
        "\n",
        "from tag_id_by_name import TAG_ID\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rf8BshEBe7G"
      },
      "source": [
        "def special_pattern_sub(doc):\n",
        "    doc = re.sub(PATTERN_HTMLTAG,'', doc)\n",
        "    doc = re.sub(PATTERN_NOT_PUNC_WSPACE_ALNUM,'',doc)\n",
        "    doc = re.sub(PATTERN_URL,'LINK',doc)\n",
        "    doc = re.sub(PATTERN_NUMBER,'NUM',doc)\n",
        "    doc = re.sub(PATTERN_PHONENB,'PNUM',doc)\n",
        "    doc = re.sub(PATTERN_EMAIL, 'EMAIL', doc)\n",
        "    doc = re.sub(PATTERN_LINEBRK,'',doc)\n",
        "\n",
        "    doc = re.sub(r\"(\\b\\D+)\\.(\\D+\\b)\",r\"\\1 . \\2\",doc)\n",
        "    return doc\n",
        "\n",
        "def preprocess(doc):\n",
        "    doc = '' if not doc else doc \n",
        "    preprocessed_doc = doc.lower() \n",
        "    preprocessed_doc = special_pattern_sub(doc)\n",
        "    \n",
        "    preprocessed_doc = __tokenize_single_doc(preprocessed_doc) \n",
        "    preprocessed_doc = unicodedata.normalize('NFKC',preprocessed_doc) \n",
        "    return doc\n",
        "\n",
        "def get_right_elem(text_list):\n",
        "    no_element = len(text_list)\n",
        "    if no_element > 1:\n",
        "        return text_list[1]\n",
        "    elif no_element == 1: \n",
        "        return text_list[0]\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "def __tokenize_single_doc(doc):\n",
        "    return \" \".join(doc.split())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXErloDnI8mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2572e9-ab94-4211-dc50-ed240037e3ce"
      },
      "source": [
        "#DATA EXPLAINATION\n",
        "\n",
        "{\n",
        "    'search_text': 'A List that usually includes 3 items, the main content usually is at 2nd position.\\n ',\n",
        "    'topic_id' : 'An int that determine the topic that the text grouped into based on keyword matching',\n",
        "    'sentiment': '1: Positive; 0: Neutral; -1: Negative; 10:undetermined',\n",
        "    'mention_type':'1: Post; 2: Comment; 3: Share',\n",
        "    'tags' : 'labeled list of tag_id; Use'\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'search_text': 'A List that usually includes 3 items, the main content usually is at 2nd position.\\n ',\n",
              " 'topic_id': 'An int that determine the topic that the text grouped into based on keyword matching',\n",
              " 'sentiment': '1: Positive; 0: Neutral; -1: Negative; 10:undetermined',\n",
              " 'mention_type': '1: Post; 2: Comment; 3: Share',\n",
              " 'tags': 'labeled list of tag_id; Use'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80zfHIxk3Awc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "931daf91-4860-41d8-e8c7-529f1c19f783"
      },
      "source": [
        "train_df = pd.read_csv(PATH_TO_TRAIN)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(df):\n",
        "    df.fillna({'search_text':'[]'}, inplace=True)\n",
        "    df['list_text'] = df.search_text.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(ast.literal_eval)\n",
        "    df['mention'] = df.list_text.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(get_right_elem)\n",
        "    df['mention'] = df.mention.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(preprocess)\n",
        "    df['mention'] = df.mention.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(lambda x: unicodedata.normalize('NFKC',x))\n",
        "\n",
        "df = train_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dask Apply: 100%|██████████| 20/20 [00:43<00:00,  2.17s/it]\n"
          ]
        }
      ],
      "source": [
        "df.fillna({'search_text':'[]'}, inplace=True)\n",
        "df['list_text'] = df.search_text.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(ast.literal_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dask Apply: 100%|██████████| 20/20 [00:13<00:00,  1.49it/s]\n"
          ]
        }
      ],
      "source": [
        "df['mention'] = df.list_text.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(get_right_elem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dask Apply: 100%|██████████| 20/20 [01:08<00:00,  3.41s/it]\n"
          ]
        }
      ],
      "source": [
        "df['mention_1'] = df.mention.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dask Apply: 100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n"
          ]
        }
      ],
      "source": [
        "df['mention_2'] = df.mention_1.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(lambda x: unicodedata.normalize('NFKC',x))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4d7_ZrPBeMT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "7342bf38-00c3-4969-f999-1ed38a0c9ddb"
      },
      "source": [
        "df.drop_duplicates(subset = ['mention_2'], inplace = True)\n",
        "# test_df.drop_duplicates(subset = ['mention'], inplace = True)\n",
        "\n",
        "\n",
        "print('Train Label Distribution: %s\\n'%df.label.value_counts())\n",
        "# print(('Test Label Distribution: %s\\n'%test_df.label.value_counts()))\n",
        "\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(df.mention_2, df.label, test_size = 0.3)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Label Distribution: 0    2686429\n1      29261\nName: label, dtype: int64\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbg-M8a8Qtb0"
      },
      "source": [
        "#BASELINE MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9QVnwyKqa3k"
      },
      "source": [
        "text_clf = Pipeline([\n",
        "                     ('tfidf',TfidfVectorizer()),\n",
        "                     ('clf',LogisticRegression())\n",
        "])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUfJPOYMVYq"
      },
      "source": [
        "param_grid = [\n",
        "              {\n",
        "                  'tfidf__ngram_range':[(1,2)],\n",
        "                  'tfidf__min_df':[0, 0.05],\n",
        "                  'tfidf__max_df':[1, 0.9],\n",
        "                  'tfidf__max_features':[3000, 4000, 5000],\n",
        "                  'clf__class_weight':['balanced'],\n",
        "                  'clf__max_iter':[200]\n",
        "              }\n",
        "]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-HEQYUVKRU3"
      },
      "source": [
        "#This step take a long time to run, possibly use RandomizedSearchCV\n",
        "GridCV = GridSearchCV(text_clf, param_grid, verbose = 1, n_jobs = -1, scoring = 'roc_auc', return_train_score=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 16.7min\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 39.9min finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                                       ('clf', LogisticRegression())]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{'clf__class_weight': ['balanced'],\n",
              "                          'clf__max_iter': [200], 'tfidf__max_df': [1, 0.9],\n",
              "                          'tfidf__max_features': [3000, 4000, 5000],\n",
              "                          'tfidf__min_df': [0, 0.05],\n",
              "                          'tfidf__ngram_range': [(1, 2)]}],\n",
              "             return_train_score=True, scoring='roc_auc', verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "GridCV.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ffjCdpKRpZ"
      },
      "source": [
        "best_clf = GridCV.best_estimator_\n",
        "best_clf"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf',\n",
              "                 TfidfVectorizer(max_df=0.9, max_features=5000, min_df=0,\n",
              "                                 ngram_range=(1, 2))),\n",
              "                ('clf',\n",
              "                 LogisticRegression(class_weight='balanced', max_iter=200))])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  clf__class_weight  clf__max_iter  tfidf__max_df  tfidf__max_features  \\\n",
              "0          balanced            200            0.9                 5000   \n",
              "1          balanced            200            0.9                 5000   \n",
              "\n",
              "   tfidf__min_df  tfidf__ngram_range  \n",
              "0              0                   1  \n",
              "1              0                   2  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clf__class_weight</th>\n      <th>clf__max_iter</th>\n      <th>tfidf__max_df</th>\n      <th>tfidf__max_features</th>\n      <th>tfidf__min_df</th>\n      <th>tfidf__ngram_range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>balanced</td>\n      <td>200</td>\n      <td>0.9</td>\n      <td>5000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>balanced</td>\n      <td>200</td>\n      <td>0.9</td>\n      <td>5000</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "pd.DataFrame(GridCV.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from meilibs import utils\n",
        "\n",
        "utils.dump_pkl('model/best_cls', GridCV.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BDrsCBRKRtC"
      },
      "source": [
        "print(classification_report(test_df.label, best_clf.predict(test_df.mention)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
