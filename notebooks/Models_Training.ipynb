{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\Uyen\\\\Documents\\\\nlp\\\\thesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_classification\n",
    "from text_classification.DataLoad import load_data\n",
    "from text_classification.TextClassificationModel import TextClassificationModel\n",
    "\n",
    "from text_classification.models import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "cf = config.Configurate()\n",
    "data_path = cf[\"Datapath\"][\"Root\"]\n",
    "train_file = cf[\"Datapath\"][\"Train\"]\n",
    "test_file = cf[\"Datapath\"][\"Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# importlib.reload(text_classification.DataLoad)\n",
    "# import YndataPreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of Text Vocabulary: 2141245\nVector size of Text Vocabulary:  torch.Size([2141245, 300])\nLabel Length: 2\n"
     ]
    }
   ],
   "source": [
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data(path=data_path, train=train_file, test=test_file, tokenizer=lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "output_size = 2\n",
    "hidden_size = 256\n",
    "embedding_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LSTM.LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1, Idx: 1000, Training Loss: 0.0105, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 2000, Training Loss: 0.0156, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 3000, Training Loss: 0.1554, Training Accuracy:  96.88%\n",
      "Epoch: 1, Idx: 4000, Training Loss: 0.0092, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 5000, Training Loss: 0.0084, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 6000, Training Loss: 0.2735, Training Accuracy:  93.75%\n",
      "Epoch: 1, Idx: 7000, Training Loss: 0.0094, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 8000, Training Loss: 0.3112, Training Accuracy:  93.75%\n",
      "Epoch: 1, Idx: 9000, Training Loss: 0.0140, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 10000, Training Loss: 0.0071, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 11000, Training Loss: 0.0086, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 12000, Training Loss: 0.0087, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 13000, Training Loss: 0.1599, Training Accuracy:  96.88%\n",
      "Epoch: 1, Idx: 14000, Training Loss: 0.0114, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 15000, Training Loss: 0.0102, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 16000, Training Loss: 0.1589, Training Accuracy:  96.88%\n",
      "Epoch: 1, Idx: 17000, Training Loss: 0.0073, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 18000, Training Loss: 0.1555, Training Accuracy:  96.88%\n",
      "Epoch: 1, Idx: 19000, Training Loss: 0.0080, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 20000, Training Loss: 0.0118, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 21000, Training Loss: 0.0079, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 22000, Training Loss: 0.0090, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 23000, Training Loss: 0.0111, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 24000, Training Loss: 0.0089, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 25000, Training Loss: 0.0091, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 26000, Training Loss: 0.0104, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 27000, Training Loss: 0.0090, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 28000, Training Loss: 0.0091, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 29000, Training Loss: 0.1529, Training Accuracy:  96.88%\n",
      "Epoch: 1, Idx: 30000, Training Loss: 0.0128, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 31000, Training Loss: 0.0098, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 32000, Training Loss: 0.0106, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 33000, Training Loss: 0.0109, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 34000, Training Loss: 0.0101, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 35000, Training Loss: 0.0109, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 36000, Training Loss: 0.0112, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 37000, Training Loss: 0.0099, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 38000, Training Loss: 0.0092, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 39000, Training Loss: 0.0106, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 40000, Training Loss: 0.0144, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 41000, Training Loss: 0.2917, Training Accuracy:  93.75%\n",
      "Epoch: 1, Idx: 42000, Training Loss: 0.0098, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 43000, Training Loss: 0.0093, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 44000, Training Loss: 0.1525, Training Accuracy:  96.88%\n",
      "Epoch: 1, Idx: 45000, Training Loss: 0.0097, Training Accuracy:  100.00%\n",
      "Epoch: 1, Idx: 46000, Training Loss: 0.1542, Training Accuracy:  96.88%\n",
      "Epoch: 01, Train Loss: 0.058, Train Acc: 98.97%, Val. Loss: 0.057153, Val. Acc: 98.97%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    train_loss, train_acc = model.train(train_iter, epoch)\n",
    "    val_loss, val_acc = model.eval(valid_iter, loss_fn)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (word_embeddings): Embedding(2141245, 300)\n",
       "  (lstm): LSTM(300, 256)\n",
       "  (label): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not str",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e128b0e12b96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nay giá bao nhiêu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\nlp\\thesis\\text_classification\\models\\LSTM.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_sentence, batch_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;34m''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embedddins.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# embedded input of shape = (batch_size, num_sequences,  embedding_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# input.size() = (num_sequences, batch_size, embedding_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "model._model(\"Nay giá bao nhiêu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}