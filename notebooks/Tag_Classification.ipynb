{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tag_Classification_Younet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct3Vi23YqEq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06e4f57-7632-4afc-faab-8490bf6789d1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "from numpy import hstack\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "import swifter\n",
        "import string\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "import ast\n",
        "from Regex import *\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Uyen\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXV1gjGz8BOa"
      },
      "source": [
        "# CONSTANT VARIABLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMlrzC5t74Ps"
      },
      "source": [
        "PATH_TO_TRAIN = 'data/PRICE_MAY_NOV_2019.csv'\n",
        "PREDICTION_THRESHOLD = 0.5\n",
        "\n",
        "from tag_id_by_name import TAG_ID\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rf8BshEBe7G"
      },
      "source": [
        "def special_pattern_sub(doc):\n",
        "    doc = re.sub(PATTERN_HTMLTAG,'', doc)\n",
        "    doc = re.sub(PATTERN_NOT_PUNC_WSPACE_ALNUM,'',doc)\n",
        "    doc = re.sub(PATTERN_URL,'LINK',doc)\n",
        "    doc = re.sub(PATTERN_NUMBER,'NUM',doc)\n",
        "    doc = re.sub(PATTERN_PHONENB,'PNUM',doc)\n",
        "    doc = re.sub(PATTERN_EMAIL, 'EMAIL', doc)\n",
        "    doc = re.sub(PATTERN_LINEBRK,'',doc)\n",
        "\n",
        "    doc = re.sub(r\"(\\b\\D+)\\.(\\D+\\b)\",r\"\\1 . \\2\",doc)\n",
        "    return doc\n",
        "\n",
        "def preprocess(doc):\n",
        "    doc = '' if not doc else doc \n",
        "    preprocessed_doc = doc.lower() \n",
        "    preprocessed_doc = special_pattern_sub(doc)\n",
        "    \n",
        "    preprocessed_doc = __tokenize_single_doc(preprocessed_doc) \n",
        "    preprocessed_doc = unicodedata.normalize('NFKC',preprocessed_doc) \n",
        "    return doc\n",
        "\n",
        "def get_right_elem(text_list):\n",
        "    no_element = len(text_list)\n",
        "    if no_element > 1:\n",
        "        return text_list[1]\n",
        "    elif no_element == 1: \n",
        "        return text_list[0]\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "def __tokenize_single_doc(doc):\n",
        "    return \" \".join(doc.split())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXErloDnI8mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2572e9-ab94-4211-dc50-ed240037e3ce"
      },
      "source": [
        "#DATA EXPLAINATION\n",
        "\n",
        "{\n",
        "    'search_text': 'A List that usually includes 3 items, the main content usually is at 2nd position.\\n ',\n",
        "    'topic_id' : 'An int that determine the topic that the text grouped into based on keyword matching',\n",
        "    'sentiment': '1: Positive; 0: Neutral; -1: Negative; 10:undetermined',\n",
        "    'mention_type':'1: Post; 2: Comment; 3: Share',\n",
        "    'tags' : 'labeled list of tag_id; Use'\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'search_text': 'A List that usually includes 3 items, the main content usually is at 2nd position.\\n ',\n",
              " 'topic_id': 'An int that determine the topic that the text grouped into based on keyword matching',\n",
              " 'sentiment': '1: Positive; 0: Neutral; -1: Negative; 10:undetermined',\n",
              " 'mention_type': '1: Post; 2: Comment; 3: Share',\n",
              " 'tags': 'labeled list of tag_id; Use'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80zfHIxk3Awc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "931daf91-4860-41d8-e8c7-529f1c19f783"
      },
      "source": [
        "train_df = pd.read_csv(PATH_TO_TRAIN)\n",
        "train_df = train_df.sample(frac=0.2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(df):\n",
        "    df.fillna({'search_text':'[]'}, inplace=True)\n",
        "    df['list_text'] = df.search_text.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(ast.literal_eval)\n",
        "    df['mention'] = df.list_text.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(get_right_elem)\n",
        "    df['mention'] = df.mention.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(preprocess)\n",
        "    df['mention'] = df.mention.swifter.set_npartitions(20).allow_dask_on_strings(enable=True).apply(lambda x: unicodedata.normalize('NFKC',x))\n",
        "\n",
        "df = train_df.copy()\n",
        "df = preprocess_text(df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4d7_ZrPBeMT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "7342bf38-00c3-4969-f999-1ed38a0c9ddb"
      },
      "source": [
        "df.drop_duplicates(subset = ['mention'], inplace = True)\n",
        "# test_df.drop_duplicates(subset = ['mention'], inplace = True)\n",
        "\n",
        "\n",
        "print('Train Label Distribution: %s\\n'%df.label.value_counts())\n",
        "# print(('Test Label Distribution: %s\\n'%test_df.label.value_counts()))\n",
        "\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(df.mention, df.label, test_size = 0.3)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Label Distribution: 0    543315\n1      6020\nName: label, dtype: int64\n\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#BASELINE MODEL\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Cbg-M8a8Qtb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9QVnwyKqa3k"
      },
      "source": [
        "text_clf = Pipeline([\n",
        "                     ('tfidf',TfidfVectorizer()),\n",
        "                     ('clf',LogisticRegression())\n",
        "])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUfJPOYMVYq"
      },
      "source": [
        "param_grid = [\n",
        "              {\n",
        "                  'tfidf__ngram_range':[(1, 3)],\n",
        "                  'tfidf__min_df':[0, 0.2],\n",
        "                  'tfidf__max_df':[1, 0.8],\n",
        "                  'tfidf__max_features':[500, 5000],\n",
        "                  'clf__class_weight':['balanced',{1:10,0:1},{1:5,0:1}],\n",
        "              }\n",
        "]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-HEQYUVKRU3"
      },
      "source": [
        "#This step take a long time to run, possibly use RandomizedSearchCV\n",
        "GridCV = GridSearchCV(text_clf, param_grid, verbose = 1, n_jobs = -1, scoring = 'roc_auc', return_train_score=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "source": [
        "GridCV.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.3min finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                                       ('clf', LogisticRegression())]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{'clf__class_weight': ['balanced'],\n",
              "                          'tfidf__max_df': [1], 'tfidf__max_features': [500],\n",
              "                          'tfidf__min_df': [0],\n",
              "                          'tfidf__ngram_range': [(1, 3)]}],\n",
              "             verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ffjCdpKRpZ"
      },
      "source": [
        "best_clf = GridCV.best_estimator_\n",
        "best_clf"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf',\n",
              "                 TfidfVectorizer(max_df=1, max_features=500, min_df=0,\n",
              "                                 ngram_range=(1, 3))),\n",
              "                ('clf', LogisticRegression(class_weight='balanced'))])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  clf__class_weight  tfidf__max_df  tfidf__max_features  tfidf__min_df  \\\n",
              "0          balanced              1                  500              0   \n",
              "1          balanced              1                  500              0   \n",
              "\n",
              "   tfidf__ngram_range  \n",
              "0                   1  \n",
              "1                   3  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clf__class_weight</th>\n      <th>tfidf__max_df</th>\n      <th>tfidf__max_features</th>\n      <th>tfidf__min_df</th>\n      <th>tfidf__ngram_range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>balanced</td>\n      <td>1</td>\n      <td>500</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>balanced</td>\n      <td>1</td>\n      <td>500</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "pd.DataFrame(GridCV.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from meilibs import utils\n",
        "\n",
        "utils.dump_pkl('model/best_cls', GridCV.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BDrsCBRKRtC"
      },
      "source": [
        "print(classification_report(y_eval, best_clf.predict(X_eval)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99    163003\n           1       0.00      0.00      0.00      1798\n\n    accuracy                           0.99    164801\n   macro avg       0.49      0.50      0.50    164801\nweighted avg       0.98      0.99      0.98    164801\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}
